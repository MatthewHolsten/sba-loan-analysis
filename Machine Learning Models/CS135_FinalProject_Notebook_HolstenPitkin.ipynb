{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Should This Loan Be Approved Or Denied: An Exploration Of Risk Classification Using SBA Loan Data**\n",
        "\n",
        "Matt Holsten, Rob Pitkin\n",
        "\n",
        "Tufts University, Spring 2022\n",
        "\n",
        "CS-135 Machine Learning, Final Project\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# This Notebook\n",
        "This Notebook is part of our SBA Loan Analysis Project described below.\n",
        "Look on our [GitHub Repository](https://github.com/MatthewHolsten/sba-loan-risk-analysis) to see the rest of the project, including the [Web App](https://matthewholsten.github.io/sba-loan-risk-analysis-webapp/) and [API](https://matthewholsten.pythonanywhere.com/) this model was used to help create.\n",
        "\n",
        "\n",
        "# Project Abstract\n",
        "\n",
        "The U.S. Small Business Administration (\"SBA\") has created a large dataset of SBA-guaranteed loans stretching back almost 60 years which documents the information and outcomes of loans. Due to the nature of the SBA and the successes they've helped create, there is an ever-growing desire to have additional risk information for predicting whether or not a borrowing-business will default on their loan.\n",
        "\n",
        "To address this desire, we first employed a feature analysis to discover which aspects of SBA-backed loans contribute most to the risk of defaulting. Then, we confirmed our analysis with two classification methods used to predict the outcomes of loans before they are granted: one linear, logistic regression model and one non-linear, feed-forward neural network. \n",
        "\n",
        "Lastly, we created a free, open source API and web application (both in our github) to deploy our machine learning models. We find that there are 11 significant features which strongly indicate loan risk, with the loan term length having the highest correlation. We also find that both linear and non-linear models produce accuracies around 70% and 80% respectively, thereby demonstrating how our applications can serve as viable tools for SBA Loan Officers, lenders, and small businesses alike.\n",
        "\n"
      ],
      "metadata": {
        "id": "dfHUDVPARuwo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXD9tc0-FmW2"
      },
      "source": [
        "#Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PMg1-viDch4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"SBAnational.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCJjEJ0LFSKo"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SYX2YcYI1il"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3LrddUN-8NT"
      },
      "outputs": [],
      "source": [
        "MA_df = df.loc[df['State'] == 'MA']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qJdz9AYC9GJ"
      },
      "outputs": [],
      "source": [
        "print(MA_df.shape)\n",
        "MA_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXhhQ5cOtw93"
      },
      "outputs": [],
      "source": [
        "CA_df = df.loc[df['State'] == 'CA']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(CA_df.shape)\n",
        "CA_df.head()"
      ],
      "metadata": {
        "id": "u0OX_8PiUB6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkdtUPkzDwtF"
      },
      "source": [
        "# Cleaning up data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RL8uL6UcKz7"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def clean_data(data):\n",
        "    # Creating a new column for unix time instead of day-month-year\n",
        "    data['ApprovalUnixTime'] = data['ApprovalDate'].apply(\n",
        "        lambda x: int(datetime.strptime(x, '%d-%b-%y').strftime(\"%s\")))\n",
        "\n",
        "    # Creating the recession feature (if the loan occurred during the Great\n",
        "    # Recession)\n",
        "    recession_start_unix = 1196485200\n",
        "    recession_end_unix = 1243828800\n",
        "    data['Recession'] = data['ApprovalUnixTime'].apply(\n",
        "        lambda x: 1 if x >= recession_start_unix and x <= recession_end_unix\n",
        "        else 0)\n",
        "\n",
        "    # Fixing approval FY column (first getting rid of error value, then\n",
        "    # converting to ints)\n",
        "    data.loc[:,'ApprovalFY'] = data['ApprovalFY'].replace('1976A', 1976)\n",
        "    data['ApprovalFY'] = data['ApprovalFY'].apply(int)\n",
        "\n",
        "\n",
        "    # Mapping years to political party\n",
        "    admin_yr = {\n",
        "        1958: 'R', 1959: 'R', 1960: 'R', 1961: 'D',\n",
        "        1962: 'D', 1963: 'D', 1964: 'D', 1965: 'D',\n",
        "        1966: 'D', 1967: 'D', 1968: 'D', 1969: 'R',\n",
        "        1970: 'R', 1971: 'R', 1972: 'R', 1973: 'R',\n",
        "        1974: 'R', 1975: 'R', 1976: 'R', 1976: 'R', \n",
        "        1977: 'D', 1978: 'D', 1979: 'D', 1980: 'D',\n",
        "        1981: 'R', 1982: 'R', 1983: 'R', 1984: 'R', \n",
        "        1985: 'R', 1986: 'R', 1987: 'R', 1988: 'R', \n",
        "        1989: 'R', 1990: 'R', 1991: 'R', 1992: 'R', \n",
        "        1993: 'D', 1994: 'D', 1995: 'D', 1996: 'D',\n",
        "        1997: 'D', 1998: 'D', 1999: 'D', 2000: 'D',\n",
        "        2001: 'R', 2002: 'R', 2003: 'R', 2004: 'R',\n",
        "        2005: 'R', 2006: 'R', 2007: 'R', 2008: 'R',\n",
        "        2009: 'D', 2010: 'D', 2011: 'D', 2012: 'D',\n",
        "        2013: 'D', 2014: 'D', 2015: 'D', 2016: 'D',\n",
        "        2020: 'D', 2021: 'D', 2022: 'D', 2023: 'D'\n",
        "    }\n",
        "\n",
        "    # Adding admin party column\n",
        "    data['AdminParty'] = data['ApprovalFY'].apply(\n",
        "        lambda x: 1 if admin_yr[x] == 'D' else 0\n",
        "    )\n",
        "\n",
        "    # Adding industry column\n",
        "    data['Industry'] = data['NAICS'].apply(\n",
        "        lambda x: int(x/10000)\n",
        "    )\n",
        "\n",
        "    # Adding real estate column\n",
        "    data['RealEstate'] = data['Term'].apply(\n",
        "        lambda x: 1 if x >= 240 else 0\n",
        "    )\n",
        "\n",
        "    # Adding SBA backed proportion column\n",
        "    props = []\n",
        "    for i in data.index:\n",
        "        props.append(float(data['SBA_Appv'][i].replace('$','').replace(',','')) / \\\n",
        "                    float(data['GrAppv'][i].replace('$','').replace(',','')))\n",
        "    data['SBAProportion'] = props\n",
        "\n",
        "    # Adding Gross disbursement float column\n",
        "    data['TotalLoan'] = data['GrAppv'].apply(\n",
        "        lambda x: float(x.replace('$','').replace(',',''))\n",
        "    )\n",
        "\n",
        "    # Adding SBA loan float column\n",
        "    data['SBALoan'] = data['SBA_Appv'].apply(\n",
        "        lambda x: float(x.replace('$','').replace(',',''))\n",
        "    )\n",
        "\n",
        "    # Adding new business column --> mode replacement (mode was existing biz)\n",
        "    data['NewBusiness'] = data['NewExist'].apply(\n",
        "        lambda x: 1 if x == 2 else 0\n",
        "    )\n",
        "\n",
        "    # Mapping states to integers based on alphabetical order\n",
        "    states = {\n",
        "        'AK': 1, 'AL': 2, 'AR': 3, 'AZ': 4,\n",
        "        'CA': 5, 'CO': 6, 'CT': 7, 'DC': 8,\n",
        "        'DE': 9, 'FL': 10, 'GA': 11, 'HI': 12,\n",
        "        'IA': 13, 'ID': 14, 'IL': 15, 'IN': 16,\n",
        "        'KS': 17, 'KY': 18, 'LA': 19, 'MA': 20,\n",
        "        'MD': 21, 'ME': 22, 'MI': 23, 'MN': 24,\n",
        "        'MO': 25, 'MS': 26, 'MT': 27, 'NC': 28,\n",
        "        'ND': 29, 'NE': 30, 'NH': 31, 'NJ': 32,\n",
        "        'NM': 33, 'NV': 34, 'NY': 35, 'OH': 36,\n",
        "        'OK': 37, 'OR': 38, 'PA': 39, 'RI': 40,\n",
        "        'SC': 41, 'SD': 42, 'TN': 43, 'TX': 44,\n",
        "        'UT': 45, 'VA': 46, 'VT': 47, 'WA': 48,\n",
        "        'WI': 49, 'WV': 50, 'WY': 51\n",
        "    }\n",
        "\n",
        "    # Adding location --> mode replacement (mode is CA)\n",
        "    data['StateNum'] = data['State'].apply(\n",
        "        lambda x: states[x] if x in states else 5\n",
        "    )\n",
        "\n",
        "    # Adding target --> mode replacement (mode is Paid In Full)\n",
        "    data['PaidOff'] = data['MIS_Status'].apply(\n",
        "        lambda x: 1 if x != 'CHGOFF' else 0\n",
        "    )\n",
        "\n",
        "    data.head()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yISfDk3ZczfD"
      },
      "outputs": [],
      "source": [
        "data = clean_data(df.copy())\n",
        "\n",
        "# data = clean_data(MA_df.copy())\n",
        "\n",
        "# data = clean_data(CA_df.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJVaRo85xW0y"
      },
      "source": [
        "# Checking unique values of columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KfRPOnNdvLo"
      },
      "outputs": [],
      "source": [
        "def print_uniques(data):\n",
        "    # Our target is Paid off\n",
        "    print(\"PaidOff\\n\", data['PaidOff'].unique(), \"\\n\")\n",
        "\n",
        "    \"\"\"\n",
        "       Our features are: Industry, Gross Disbursement, \n",
        "       New vs. Established business, Loans backed by real estate, \n",
        "       Economic recession, SBA's guaranteed portion, SBA backed proportion,\n",
        "       Administration Party, Urban vs. Rural, Jobs Created, Jobs Retained,\n",
        "       and Loan Term\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Industry\\n\",data['Industry'].unique(), \"\\n\")\n",
        "    print(\"TotalLoan\\n\",data['TotalLoan'].unique(), \"\\n\")\n",
        "    print(\"New vs. Old\\n\",data['NewBusiness'].unique(), \"\\n\")\n",
        "    print(\"RealEstate\\n\",data['RealEstate'].unique(), \"\\n\")\n",
        "    print(\"Recession\\n\",data['Recession'].unique(), \"\\n\")\n",
        "    print(\"SBALoan\\n\",data['SBALoan'].unique(), \"\\n\")\n",
        "    print(\"SBAProportion\\n\",data['SBAProportion'].unique(), \"\\n\")\n",
        "    print(\"AdminParty\\n\",data['AdminParty'].unique(), \"\\n\")\n",
        "    print(\"UrbanRural\\n\",data['UrbanRural'].unique(), \"\\n\")\n",
        "    print(\"CreateJob\\n\",data['CreateJob'].unique(), \"\\n\")\n",
        "    print(\"RetainedJob\\n\",data['RetainedJob'].unique(), \"\\n\")\n",
        "    print(\"Term\\n\",data['Term'].unique(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSWvNWiyd-D-"
      },
      "outputs": [],
      "source": [
        "print_uniques(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T33Dt_4o5Ji"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaBava2DeNoW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def perform_feature_selection(data, include_loc=False):\n",
        "\n",
        "    # Isolating target variable (Paid in full vs. Charged Off)\n",
        "    target = data.copy().iloc[:,37]\n",
        "\n",
        "    # Counting the number of positive examples vs. negative examples\n",
        "    ones = 0\n",
        "    for t in target:\n",
        "        if t == 1:\n",
        "            ones += 1\n",
        "    ones = ones/target.size\n",
        "    zeros = 1 - ones\n",
        "\n",
        "    weights = [max(ones, zeros)/zeros, max(ones,zeros)/ones]\n",
        "\n",
        "    # Dictionary of features to column indices\n",
        "    features = {\n",
        "        'Industry': 30,\n",
        "        'TotalLoan': 33,\n",
        "        'NewBusiness': 35,\n",
        "        'RealEstate': 31,\n",
        "        'Recession': 28,\n",
        "        'SBALoan': 34,\n",
        "        'AdminParty': 29,\n",
        "        'SBAProportion': 32,\n",
        "        'UrbanRural' : 16,\n",
        "        'CreateJob' : 13,\n",
        "        'RetainedJob' : 14,\n",
        "        'Term' : 10,\n",
        "        'StateNum' : 36\n",
        "    }\n",
        "\n",
        "    # Creating a list of feature columns\n",
        "    fs = []\n",
        "    fs_names = []\n",
        "    for x in features:\n",
        "        fs_names.append(x)\n",
        "        fs.append(np.array(data.copy().iloc[:,features[x]]))\n",
        "    if not include_loc:\n",
        "        fs.pop(-1)\n",
        "        fs_names.pop(-1)\n",
        "    fs = np.array(fs).T\n",
        "    print(\"Number of features:\",fs.shape[1])\n",
        "\n",
        "    # Feature selection: MI test and F-test\n",
        "    if include_loc:\n",
        "        mutual_info = mutual_info_classif(fs, target, discrete_features=[1,0,1,1,1,0,1,0,1,0,0,0,1])\n",
        "        f_test, _ = f_classif(fs, target)\n",
        "    else:\n",
        "        mutual_info = mutual_info_classif(fs, target, discrete_features=[1,0,1,1,1,0,1,0,1,0,0,0])\n",
        "        f_test, _ = f_classif(fs, target)\n",
        "    return fs, fs_names, target, mutual_info, f_test, weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6NrhU8CelzM",
        "outputId": "5aeb3618-6fe6-48d2-bddc-578dbaa9d81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features: 13\n"
          ]
        }
      ],
      "source": [
        "# Don't include location (state-level)\n",
        "# fs, fs_names, target, mi, ftest, weights = perform_feature_selection(data, False)\n",
        "\n",
        "# Include location (country-wide)\n",
        "fs, fs_names, target, mi, ftest, weights = perform_feature_selection(data, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mutual Information Test"
      ],
      "metadata": {
        "id": "Gcl-OOI-fdlx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v4abAH0fOBn"
      },
      "outputs": [],
      "source": [
        "def print_MI_results(fs_names, mutual_info):\n",
        "    # Mutual information results\n",
        "    print(\"Mutual Information by index: \", mutual_info)\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(14.5, 8.5)\n",
        "    plt.bar(fs_names, mutual_info)\n",
        "    plt.title(\"Features vs. Mutual Information\")\n",
        "    plt.xlabel(\"Feature\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel(\"Mutual Information Value\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp-rC_T9fenH"
      },
      "outputs": [],
      "source": [
        "print_MI_results(fs_names, mi)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F-Test"
      ],
      "metadata": {
        "id": "TNebWCXvfb8z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vBS_R8GseBv"
      },
      "outputs": [],
      "source": [
        "def print_ftest_results(fs_names, f_test):\n",
        "    # F-test results\n",
        "    print(\"F-values by index: \", f_test)\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(14.5, 8.5)\n",
        "    plt.bar(fs_names, f_test)\n",
        "    plt.title(\"Features vs. F-Test value\")\n",
        "    plt.xlabel(\"Feature\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel(\"F-value\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0vrMO5bslUK"
      },
      "outputs": [],
      "source": [
        "print_ftest_results(fs_names, ftest)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Matrix"
      ],
      "metadata": {
        "id": "7MspdA9yfhRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdzU5kk-P4B0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sn\n",
        "\n",
        "def create_correlation_matrix(feature_cols, feature_names, feature_target):\n",
        "    corr_df = pd.DataFrame(feature_cols, columns=feature_names)\n",
        "    corr_df['PaidOff'] = feature_target\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(18.5, 10.5)\n",
        "    corr_matrix = corr_df.corr()\n",
        "    sn.heatmap(corr_matrix, annot=True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_correlation_matrix(fs, fs_names, target)"
      ],
      "metadata": {
        "id": "NVE8QQvGk51U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8nGqwUnC_Qy"
      },
      "source": [
        "# Separating test and training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbxJOSLsgQcv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def create_train_test_sets(X, Y, weights):\n",
        "    # Getting rid of 'NewBusiness'\n",
        "    features = np.delete(X, 2, 1)\n",
        "\n",
        "    # Getting rid of 'CreateJob'\n",
        "    features = np.delete(features, 8, 1)\n",
        "\n",
        "    # Normalizing total loan, sba loan, sba proportion, retained jobs,\n",
        "    # and loan term\n",
        "    means = []\n",
        "    std_devs = []\n",
        "    sc = StandardScaler()\n",
        "\n",
        "    # Total Loan\n",
        "    features[:,1] = sc.fit_transform(features[:,1].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # SBA Loan\n",
        "    features[:,4] = sc.fit_transform(features[:,4].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # SBA Proportion\n",
        "    features[:,6] = sc.fit_transform(features[:,6].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # Retained Jobs\n",
        "    features[:,8] = sc.fit_transform(features[:,8].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # Loan Term\n",
        "    features[:,9] = sc.fit_transform(features[:,9].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # Creating test and training sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(features, Y, test_size=0.25, random_state=73)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.13, random_state=81)\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "    y_val = np.array(y_val)\n",
        "\n",
        "    w_train = [weights[0] if x == 0 else weights[1] for x in y_train]\n",
        "    w_test = [weights[0] if x == 0 else weights[1] for x in y_test]\n",
        "\n",
        "    # Converting sets to tensors\n",
        "    x_train = torch.tensor(x_train.astype(np.float32))\n",
        "    x_test = torch.tensor(x_test.astype(np.float32))\n",
        "    x_val = torch.tensor(x_val.astype(np.float32))\n",
        "    y_train = torch.tensor(np.around(y_train.astype(np.float32)))\n",
        "    y_test = torch.tensor(np.around(y_test.astype(np.float32)))\n",
        "    y_val = torch.tensor(np.around(y_val.astype(np.float32)))\n",
        "    \n",
        "    data_sets = {\n",
        "        \"x_train\" : x_train,\n",
        "        \"x_test\" : x_test,\n",
        "        \"x_val\" : x_val,\n",
        "        \"y_train\" : y_train,\n",
        "        \"y_test\" : y_test,\n",
        "        \"y_val\" : y_val,\n",
        "        \"w_train\" : w_train, \n",
        "        \"w_test\" : w_test, \n",
        "        \"means\" : means, \n",
        "        \"sds\" : std_devs\n",
        "    }\n",
        "\n",
        "    return data_sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWWs7HbWeIeI"
      },
      "outputs": [],
      "source": [
        "data_sets = create_train_test_sets(fs, target, weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "s4Wfpw-SfJrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our Logistic Regression class"
      ],
      "metadata": {
        "id": "R2QyTFAQeq3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcyVI6_4C9nm"
      },
      "outputs": [],
      "source": [
        "class SBALogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, means, std_devs):\n",
        "        super(SBALogisticRegression, self).__init__()\n",
        "\n",
        "        # Storing means and std_devs for future queries\n",
        "        self.means = means\n",
        "        self.std_devs = std_devs \n",
        "\n",
        "        # Creating out logistic regression unit\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)  \n",
        "   \n",
        "    def forward(self, x):\n",
        "        outputs = torch.sigmoid(self.linear(x))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR model and training function"
      ],
      "metadata": {
        "id": "tsi5pipCewco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRepochs = 50000\n",
        "input_dim = 10 # Our features\n",
        "output_dim = 1 # Single binary output \n",
        "learning_rate = 0.001\n",
        "\n",
        "LRmodel = SBALogisticRegression(input_dim, output_dim, data_sets[\"means\"], data_sets[\"sds\"])\n",
        "LR_train_loss = torch.nn.BCELoss(weight = torch.FloatTensor(data_sets[\"w_train\"]))\n",
        "LR_test_loss = torch.nn.BCELoss(weight = torch.FloatTensor(data_sets[\"w_test\"]))\n",
        "LRoptimizer = torch.optim.SGD(LRmodel.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "5n0nP8hL9vo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3Ifp3yVjOpW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "def LR_train_and_test(model, optimizer, x_train, x_test, y_train, y_test):\n",
        "    iter = 0\n",
        "    predicted_labels = None\n",
        "    final_predicted = None\n",
        "    # fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
        "    for epoch in range(int(LRepochs)):\n",
        "        iter+=1\n",
        "\n",
        "        # Zeroing our gradients for the batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed-forward pass\n",
        "        outputs = model(x_train)\n",
        "\n",
        "        # Finding our loss and computing the gradients\n",
        "        loss = LR_train_loss(torch.squeeze(outputs), y_train) \n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating weights and biases\n",
        "        optimizer.step()\n",
        "        \n",
        "        if iter % (LRepochs/50) == 0:\n",
        "            # Calculating the loss and accuracy for the test dataset\n",
        "            test_outputs = torch.squeeze(model(x_test))\n",
        "            test_loss = LR_test_loss(test_outputs, y_test)\n",
        "            \n",
        "            predicted_prob = test_outputs.detach().numpy()\n",
        "            predicted_test = test_outputs.round().detach().numpy()\n",
        "            test_correct = np.sum(predicted_test == y_test.detach().numpy())\n",
        "            test_accuracy = 100 * test_correct/y_test.size(0)\n",
        "            \n",
        "            # Calculating the loss and accuracy for the train dataset\n",
        "            train_correct = np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train.detach().numpy())\n",
        "            train_accuracy = 100 * train_correct/y_train.size(0)\n",
        "            \n",
        "            print(\"Iteration:\", iter,\"\\nTest - Loss:\", test_loss.item(), \"Accuracy:\", test_accuracy)\n",
        "            print(\"Train - Loss:\", loss.item(), \"Accuracy:\", train_accuracy, \"\\n\")\n",
        "\n",
        "            if predicted_labels is None:\n",
        "                predicted_labels = predicted_prob\n",
        "            else:\n",
        "                np.append(predicted_labels, predicted_prob)\n",
        "\n",
        "            final_predicted = predicted_test\n",
        "\n",
        "    # Build confusion matrix and ROC Curve\n",
        "    fig = plt.figure()\n",
        "    ConfusionMatrixDisplay.from_predictions(y_test.detach().numpy(), final_predicted)\n",
        "    RocCurveDisplay.from_predictions(y_test.detach().numpy(), predicted_labels)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gpgDPAajsWG"
      },
      "outputs": [],
      "source": [
        "LR_train_and_test(LRmodel, LRoptimizer, data_sets[\"x_train\"], data_sets[\"x_test\"], data_sets[\"y_train\"], data_sets[\"y_test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJw7-QJBW7fP"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our Neural Network class"
      ],
      "metadata": {
        "id": "G8LsVx7lfF8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGA7jN-uW63Y"
      },
      "outputs": [],
      "source": [
        "class SBANeuralNet(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, means, std_devs, iterations):\n",
        "        super(SBANeuralNet, self).__init__()\n",
        "        # Storing means and std_devs for future queries\n",
        "        self.means = means\n",
        "        self.std_devs = std_devs\n",
        "        self.iterations = iterations\n",
        "\n",
        "        # Fully-connected Layers\n",
        "        self.full_1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.full_2 = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Activation function\n",
        "        self.sigmoid = torch.nn.Sigmoid()  \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aggregation of layer 1\n",
        "        out = self.full_1(x)\n",
        "        # Activation of layer 1\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        # Aggregation of layer 2\n",
        "        out = self.full_2(out)\n",
        "        # Activation of layer 2\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our Neural Network model and training function"
      ],
      "metadata": {
        "id": "8h0VJ_n-fRCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzWhBoZOYZz9"
      },
      "outputs": [],
      "source": [
        "NNepochs = [5000, 10000, 20000] # Number of iterations\n",
        "input_dim = 10 # Our features\n",
        "hidden_dims = [2, 4, 8, 16, 32]  # Hidden layer sizes\n",
        "output_dim = 1 # Single binary output \n",
        "learning_rates = np.logspace(-3, -1, 5)\n",
        "\n",
        "NNmodels = []\n",
        "model_parameters = []\n",
        "for it in NNepochs:\n",
        "    for hd in hidden_dims:\n",
        "        for lr in learning_rates:\n",
        "            model_parameters.append((hd, lr, it))\n",
        "            NNmodel = SBANeuralNet(input_dim, hd, output_dim, data_sets[\"means\"], data_sets[\"sds\"], it)\n",
        "            NNoptimizer = torch.optim.SGD(NNmodel.parameters(), lr=lr, momentum=0.9)\n",
        "            NNmodels.append((NNmodel, NNoptimizer))\n",
        "\n",
        "NN_train_loss = torch.nn.BCELoss(weight = torch.FloatTensor(data_sets[\"w_train\"]))\n",
        "NN_test_loss = torch.nn.BCELoss(weight = torch.FloatTensor(data_sets[\"w_test\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ww27DZk04j"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "def NN_train_and_test(NNmodel, NNoptimizer, x_train, x_test, y_train, y_test, epochs):\n",
        "    iter = 0\n",
        "    best_accuracy = 0\n",
        "    predicted_labels = None\n",
        "    final_predicted = None\n",
        "    for epoch in range(epochs):\n",
        "        NNmodel.train(True)\n",
        "        # Zeroing our gradients for the batch\n",
        "        NNoptimizer.zero_grad()\n",
        "\n",
        "        # Feed-forward pass\n",
        "        outputs = NNmodel(x_train)\n",
        "\n",
        "         # Finding our loss and computing the gradients\n",
        "        loss = NN_train_loss(torch.squeeze(outputs), y_train) \n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating weights and biases\n",
        "        NNoptimizer.step()\n",
        "\n",
        "        iter+=1\n",
        "        if iter % 1000 == 0:\n",
        "            NNmodel.train(False)\n",
        "            # Calculating the loss and accuracy for the test dataset\n",
        "            test_outputs = torch.squeeze(NNmodel(x_test))\n",
        "            test_loss = NN_test_loss(test_outputs, y_test)\n",
        "            \n",
        "            predicted_test = test_outputs.round().detach().numpy()\n",
        "            predicted_prob = test_outputs.detach().numpy()\n",
        "            test_correct = np.sum(predicted_test == y_test.detach().numpy())\n",
        "            test_accuracy = 100 * test_correct/y_test.size(0)\n",
        "            \n",
        "            # Calculating the loss and accuracy for the train dataset\n",
        "            train_correct = np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train.detach().numpy())\n",
        "            train_accuracy = 100 * train_correct/y_train.size(0)\n",
        "            \n",
        "            print(\"Iteration:\", iter,\"\\nTesting Loss:\", test_loss.item(), \"Testing Accuracy:\", test_accuracy)\n",
        "            print(\"Training Loss:\", loss.item(), \"Training Accuracy:\", train_accuracy, \"\\n\")\n",
        "\n",
        "            if predicted_labels is None:\n",
        "                predicted_labels = predicted_prob\n",
        "            else:\n",
        "                np.append(predicted_labels, predicted_prob)\n",
        "\n",
        "            final_predicted = predicted_test\n",
        "            \n",
        "    # Build confusion matrix and ROC Curve\n",
        "    fig = plt.figure()\n",
        "    ConfusionMatrixDisplay.from_predictions(y_test.detach().numpy(), final_predicted)\n",
        "    RocCurveDisplay.from_predictions(y_test.detach().numpy(), predicted_labels)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddGKlg1mlOcg"
      },
      "outputs": [],
      "source": [
        "for m, o in NNmodels:\n",
        "    NN_train_and_test(m, o, data_sets[\"x_train\"], data_sets[\"x_test\"], data_sets[\"y_train\"], data_sets[\"y_test\"], m.iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSQP9FwfIahP"
      },
      "source": [
        "# Adding the ability to query the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usT_5uE8IeJb"
      },
      "outputs": [],
      "source": [
        "def query(model, sample, print_result=True):\n",
        "    model.train(False)\n",
        "    \n",
        "    output = torch.squeeze(model(sample))\n",
        "    output = output.round().detach().numpy()\n",
        "    if output == 1.0:\n",
        "        if print_result:\n",
        "            print(\"Predicted low risk loan\")\n",
        "        return 1.0\n",
        "    else:\n",
        "        if print_result:\n",
        "            print(\"Predicted high risk loan\")\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validating the models"
      ],
      "metadata": {
        "id": "sBMr5w_bHSwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "LUxOBUW3dqbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = 0.0\n",
        "\n",
        "for i, x in enumerate(data_sets[\"x_val\"]):\n",
        "    pred = query(LRmodel, x, False)\n",
        "    if data_sets[\"y_val\"][i].detach().numpy() == pred:\n",
        "        accuracy += 1.0\n",
        "\n",
        "print(\"Accuracy for LR:\",100*(accuracy/data_sets[\"y_val\"].size(0)))\n"
      ],
      "metadata": {
        "id": "thPkRxNiHW0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "Pl5GTJcydvtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (m, _) in enumerate(NNmodels):\n",
        "    accuracy = 0.0\n",
        "\n",
        "    for j, x in enumerate(data_sets[\"x_val\"]):\n",
        "        pred = query(m, x, False)\n",
        "        if data_sets[\"y_val\"][j].detach().numpy() == pred:\n",
        "            accuracy += 1.0\n",
        "\n",
        "    print(\"Hyper-parameters: HD1 -\", model_parameters[i][0], \"LearningRate -\", model_parameters[i][1], \"Iterations -\", model_parameters[i][2])\n",
        "    print(\"Accuracy for NN:\",100*(accuracy/data_sets[\"y_val\"].size(0)))"
      ],
      "metadata": {
        "id": "GfyujJx4NixB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and training the best model from hyper-parameter tuning"
      ],
      "metadata": {
        "id": "9L4eduMKpzP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_epochs = 20000 # Number of iterations\n",
        "input_dim = 10 # Our features\n",
        "hidden_dim = 16 # Hidden layer 1 sizes\n",
        "output_dim = 1 # Single binary output \n",
        "lr = 0.01\n",
        "\n",
        "best_model = SBANeuralNet(input_dim, hidden_dim, output_dim, data_sets[\"means\"], data_sets[\"sds\"], best_epochs)\n",
        "optimizer = torch.optim.SGD(best_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "NN_train_loss = torch.nn.BCELoss(weight = torch.FloatTensor(data_sets[\"w_train\"]))\n",
        "NN_test_loss = torch.nn.BCELoss(weight = torch.FloatTensor(data_sets[\"w_test\"]))"
      ],
      "metadata": {
        "id": "qPiphx8pp7Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_train_and_test(best_model, optimizer, data_sets[\"x_train\"], data_sets[\"x_test\"], data_sets[\"y_train\"], data_sets[\"y_test\"], best_epochs)"
      ],
      "metadata": {
        "id": "PIPBACHCqSzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the optimal model on the validation set"
      ],
      "metadata": {
        "id": "u2gmBB_Wd0uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = 0.0\n",
        "\n",
        "for j, x in enumerate(data_sets[\"x_val\"]):\n",
        "    pred = query(best_model, x, False)\n",
        "    if data_sets[\"y_val\"][j].detach().numpy() == pred:\n",
        "        accuracy += 1.0\n",
        "\n",
        "print(\"Accuracy for NN:\",100*(accuracy/data_sets[\"y_val\"].size(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiQMsoQj1KWf",
        "outputId": "00e3d72b-2f89-49c1-9c45-6dc5a98a4ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for NN: 80.85192697768763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting the Model"
      ],
      "metadata": {
        "id": "nzwGVGILDrIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model.state_dict(), \"country_model.pt\")\n",
        "# torch.save(best_model.state_dict(), \"ca_model.pt\")\n",
        "# torch.save(best_model.state_dict(), \"ma_model.pt\")"
      ],
      "metadata": {
        "id": "dYK6-q09CKd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(best_model, torch.zeros(11), 'country_model.onnx', verbose=True)\n",
        "# torch.onnx.export(best_model, torch.zeros(10), 'ca_model.onnx', verbose=True)\n",
        "# torch.onnx.export(best_model, torch.zeros(10), 'ma_model.onnx', verbose=True)"
      ],
      "metadata": {
        "id": "0P7tDLg1C5y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with fewer features"
      ],
      "metadata": {
        "id": "oPwEdNN9UUeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing features"
      ],
      "metadata": {
        "id": "xwwCO9pxeJkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(fs_names)\n",
        "\n",
        "fs_copy = fs.copy()\n",
        "fs_names_copy = fs_names.copy()\n",
        "\n",
        "# Getting rid of 'NewBusiness'\n",
        "fs_copy = np.delete(fs_copy, 2, 1)\n",
        "fs_names_copy.pop(2)\n",
        "\n",
        "# Getting rid of 'CreateJob'\n",
        "fs_copy = np.delete(fs_copy, 8, 1)\n",
        "fs_names_copy.pop(8)\n",
        "\n",
        "# Getting rid of 'TotalLoan'\n",
        "fs_copy = np.delete(fs_copy, 1, 1)\n",
        "fs_names_copy.pop(1)\n",
        "\n",
        "# Getting rid of 'SBALoan'\n",
        "fs_copy = np.delete(fs_copy, 3, 1)\n",
        "fs_names_copy.pop(3)\n",
        "\n",
        "# # Getting rid of loan term - make -2 if country-wide\n",
        "# fs_copy = np.delete(fs_copy, -2, 1)\n",
        "# fs_names_copy.pop(-2)\n",
        "\n",
        "# # Getting rid of SBA proportion\n",
        "# fs_copy = np.delete(fs_copy, 6, 1)\n",
        "# fs_names_copy.pop(6)\n",
        "\n",
        "# Getting rid of AdminParty\n",
        "# fs_copy = np.delete(fs_copy, 5, 1)\n",
        "# fs_names_copy.pop(5)\n",
        "\n",
        "print(fs_copy.shape)\n",
        "\n",
        "print(fs_names_copy)"
      ],
      "metadata": {
        "id": "n24JH785Ua50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating training, test, and validation sets with new set of features"
      ],
      "metadata": {
        "id": "8JgMXs2ceMJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def create_less_train_test_sets(X, Y, weights):\n",
        "    print(X.shape)\n",
        "\n",
        "    # Normalizing total loan, sba loan, sba proportion, retained jobs,\n",
        "    # and loan term\n",
        "    features = X\n",
        "    means = []\n",
        "    std_devs = []\n",
        "    sc = StandardScaler()\n",
        "\n",
        "    # Total Loan\n",
        "    # features[:,1] = sc.fit_transform(features[:,1].reshape(-1,1)).flatten()\n",
        "    # means.append(sc.mean_)\n",
        "    # std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # SBA Loan\n",
        "    features[:,4] = sc.fit_transform(features[:,4].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # SBA Proportion\n",
        "    # features[:,5] = sc.fit_transform(features[:,5].reshape(-1,1)).flatten()\n",
        "    # means.append(sc.mean_)\n",
        "    # std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # Retained Job\n",
        "    features[:,6] = sc.fit_transform(features[:,6].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    # Loan Term\n",
        "    features[:,7] = sc.fit_transform(features[:,7].reshape(-1,1)).flatten()\n",
        "    means.append(sc.mean_)\n",
        "    std_devs.append(math.sqrt(sc.var_))\n",
        "\n",
        "    print(features.shape)\n",
        "\n",
        "    # Creating test and training sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(features, Y, test_size=0.25, random_state=73)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.13, random_state=81)\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "    y_val = np.array(y_val)\n",
        "\n",
        "    w_train = [weights[0] if x == 0 else weights[1] for x in y_train]\n",
        "    w_test = [weights[0] if x == 0 else weights[1] for x in y_test]\n",
        "\n",
        "    # Converting sets to tensors\n",
        "    x_train = torch.tensor(x_train.astype(np.float32))\n",
        "    x_test = torch.tensor(x_test.astype(np.float32))\n",
        "    x_val = torch.tensor(x_val.astype(np.float32))\n",
        "    y_train = torch.tensor(np.around(y_train.astype(np.float32)))\n",
        "    y_test = torch.tensor(np.around(y_test.astype(np.float32)))\n",
        "    y_val = torch.tensor(np.around(y_val.astype(np.float32)))\n",
        "    \n",
        "    data_sets = {\n",
        "        \"x_train\" : x_train,\n",
        "        \"x_test\" : x_test,\n",
        "        \"x_val\" : x_val,\n",
        "        \"y_train\" : y_train,\n",
        "        \"y_test\" : y_test,\n",
        "        \"y_val\" : y_val,\n",
        "        \"w_train\" : w_train, \n",
        "        \"w_test\" : w_test, \n",
        "        \"means\" : means, \n",
        "        \"sds\" : std_devs\n",
        "    }\n",
        "\n",
        "    return data_sets"
      ],
      "metadata": {
        "id": "AjLv2jSXY2g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "less_data_sets = create_less_train_test_sets(fs_copy, target, weights)"
      ],
      "metadata": {
        "id": "woEuYZXkYpHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating and running our logistic regression model with new features and datasets"
      ],
      "metadata": {
        "id": "Ha2KG7BUeRw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRepochs = 50000\n",
        "input_dim = 9 # Our features\n",
        "output_dim = 1 # Single binary output \n",
        "learning_rate = 0.001\n",
        "\n",
        "LR_less_model = SBALogisticRegression(input_dim, output_dim, less_data_sets[\"means\"], less_data_sets[\"sds\"])\n",
        "LR_train_loss = torch.nn.BCELoss(weight = torch.FloatTensor(less_data_sets[\"w_train\"]))\n",
        "LR_test_loss = torch.nn.BCELoss(weight = torch.FloatTensor(less_data_sets[\"w_test\"]))\n",
        "LR_less_optimizer = torch.optim.SGD(LR_less_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "i4wfyeKgX3ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_train_and_test(LR_less_model, LR_less_optimizer, less_data_sets[\"x_train\"], less_data_sets[\"x_test\"], less_data_sets[\"y_train\"], less_data_sets[\"y_test\"])"
      ],
      "metadata": {
        "id": "6ymf9rCvYH06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = 0.0\n",
        "\n",
        "for i, x in enumerate(less_data_sets[\"x_val\"]):\n",
        "    pred = query(LR_less_model, x, False)\n",
        "    if less_data_sets[\"y_val\"][i].detach().numpy() == pred:\n",
        "        accuracy += 1.0\n",
        "\n",
        "print(\"Accuracy for LR:\",100*(accuracy/less_data_sets[\"y_val\"].size(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk6etAzXycqL",
        "outputId": "3695fe6c-5885-4676-fff5-6a6613e964cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for LR: 68.46091548894135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating and running our neural network with new features and datasets"
      ],
      "metadata": {
        "id": "ZE9rdDQgebdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_less_model = SBANeuralNet(9, hidden_dim, output_dim, less_data_sets[\"means\"], less_data_sets[\"sds\"], best_epochs)\n",
        "less_optimizer = torch.optim.SGD(best_less_model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "NN_train_loss = torch.nn.BCELoss(weight = torch.FloatTensor(less_data_sets[\"w_train\"]))\n",
        "NN_test_loss = torch.nn.BCELoss(weight = torch.FloatTensor(less_data_sets[\"w_test\"]))"
      ],
      "metadata": {
        "id": "CPn543AUYIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_train_and_test(best_less_model, less_optimizer, less_data_sets[\"x_train\"], less_data_sets[\"x_test\"], less_data_sets[\"y_train\"], less_data_sets[\"y_test\"], best_epochs)"
      ],
      "metadata": {
        "id": "mLYLTm_nYbdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = 0.0\n",
        "\n",
        "for j, x in enumerate(less_data_sets[\"x_val\"]):\n",
        "    pred = query(best_less_model, x, False)\n",
        "    if less_data_sets[\"y_val\"][j].detach().numpy() == pred:\n",
        "        accuracy += 1.0\n",
        "\n",
        "print(\"Accuracy for NN:\",100*(accuracy/less_data_sets[\"y_val\"].size(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuQGUzROyoSs",
        "outputId": "0d4518fa-2de0-4940-b7cd-043993416a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for NN: 80.4161105978168\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS135_FinalProject_Notebook_HolstenPitkin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}